{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff774595-67f0-420d-a377-8e5a684a79e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 documents for training.\n",
      "\n",
      "Processed 1/20: Abhijeet_Raj_vs_State_Govt_Of_Nct_Of_Delhi_on_27_April_2016.PDF\n",
      "Processed 2/20: Anbazhagan_vs_The_State_Rep_By_The_Inspector_Of_Police_on_20_July_2023.PDF\n",
      "Processed 3/20: Anda_And_Ors_vs_The_State_Of_Rajasthan_on_9_March_1965.PDF\n",
      "Processed 4/20: Ashapura_Minechem_Ltd_vs_Indian_Bureau_Of_Mines_Thr_Minstry_And_on_6_June_2025.PDF\n",
      "Processed 5/20: Board_Of_Control_For_Cricket_In_India_vs_M_S_Rendezvous_Sports_World_And_6_on_17_June_2025.PDF\n",
      "Processed 6/20: Ibra_Akanda_And_Ors_vs_Emperor_on_8_February_1944.PDF\n",
      "Processed 7/20: Jayrambhai_Panchiyabhai_Gamit_vs_State_Of_Gujarat_on_7_March_2017.PDF\n",
      "Processed 8/20: Naredco_West_Foundation_vs_Citispace_And_6_Ors_on_19_June_2025.PDF\n",
      "Processed 9/20: Ngo_Alliance_For_Governance_And_vs_State_Of_Maharashtra_And_Ors_on_19_June_2025.PDF\n",
      "Processed 10/20: Nusli_N_Wadia_And_2_Ors_vs_Bastion_Constructions_on_27_June_2025.PDF\n",
      "Processed 11/20: Pandurang_D_Chalke_And_Anr_vs_Citispace_And_Ors_on_19_June_2025.PDF\n",
      "Processed 12/20: Pramod_Kumar_vs_State_Of_U_P_on_28_February_2019.PDF\n",
      "Processed 13/20: Pratik_Jagdishbhai_Thakkar_5_vs_State_Of_Gujarat_on_5_May_2017.PDF\n",
      "Processed 14/20: Rekha_P_Thapar_And_7_Ors_vs_State_Of_Maharashtra_Through_Sr_on_10_June_2025.PDF\n",
      "Processed 15/20: Sandesh_Mahadev_Lavnde_And_3_Ors_vs_Collector_Mumbai_Suburban_District_on_6_June_2025 (1).PDF\n",
      "Processed 16/20: Sandesh_Mahadev_Lavnde_And_3_Ors_vs_Collector_Mumbai_Suburban_District_on_6_June_2025.PDF\n",
      "Processed 17/20: Slum_Redevelopers_Association_And_3_vs_Citispace_And_6_Ors_on_19_June_2025.PDF\n",
      "Processed 18/20: The_State_Of_Maharashtra_vs_Jahur_Kamruddin_Khatik_on_14_August_1997.PDF\n",
      "Processed 19/20: The_State_Of_Maharashtra_vs_Ramchandra_Sambhaji_Karanjule_on_11_March_2016.PDF\n",
      "Processed 20/20: Udai_Singh_vs_State_on_2_December_2008.PDF\n",
      "\n",
      "Computing Embeddings...\n",
      "Building Hybrid Matrix...\n",
      "\n",
      "============================================================\n",
      "RUNNING RECOMMENDER METRICS (LOO)\n",
      "============================================================\n",
      "Precision@1: 40.96%\n",
      "Recall@5:    63.86%\n",
      "MRR:         0.4886\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# BLOCK 1: TRAINING & LOO EVALUATION\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import PyPDF2\n",
    "import docx\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "ACT_PATTERNS = {\n",
    "    'IPC': [\n",
    "        r'Section\\s+(\\d+[A-Z]*)\\s+(?:of\\s+)?(?:the\\s+)?(?:Indian\\s+Penal\\s+Code|IPC)',\n",
    "        r'(?:IPC|Indian\\s+Penal\\s+Code)\\s+(?:Section\\s+)?(\\d+[A-Z]*)',\n",
    "        r'(?:u/s|under\\s+section|read\\s+with)\\s+(\\d+[A-Z]*)\\s+(?:of\\s+)?(?:the\\s+)?IPC',\n",
    "        r'(\\d+[A-Z]*)\\s+(?:IPC|Indian\\s+Penal\\s+Code)'\n",
    "    ],\n",
    "    'CrPC': [\n",
    "        r'Section\\s+(\\d+[A-Z]*)\\s+(?:of\\s+)?(?:the\\s+)?(?:Cr\\.?P\\.?C\\.?|CrPC|Criminal\\s+Procedure\\s+Code)',\n",
    "        r'(?:Cr\\.?P\\.?C\\.?|CrPC)\\s+(?:Section\\s+)?(\\d+[A-Z]*)'\n",
    "    ],\n",
    "    'Evidence_Act': [\n",
    "        r'Section\\s+(\\d+[A-Z]*)\\s+(?:of\\s+)?(?:the\\s+)?Evidence\\s+Act',\n",
    "        r'(\\d+[A-Z]*)\\s+of\\s+(?:the\\s+)?Evidence\\s+Act'\n",
    "    ],\n",
    "    'Arbitration_Act': [\n",
    "        r'Section\\s+(\\d+[A-Z]*(?:\\(\\d+\\))?)\\s+(?:of\\s+)?(?:the\\s+)?(?:ACA|Arbitration\\s+(?:and\\s+Conciliation\\s+)?Act(?:,\\s*1996)?)',\n",
    "        r'(\\d+[A-Z]*(?:\\(\\d+\\))?)\\s+of\\s+(?:the\\s+)?(?:ACA|Arbitration\\s+Act)'\n",
    "    ],\n",
    "    'Contract_Act': [\n",
    "        r'Section\\s+(\\d+[A-Z]*)\\s+(?:of\\s+)?(?:the\\s+)?(?:ICA|Indian\\s+Contract\\s+Act(?:,\\s*1872)?)',\n",
    "        r'(\\d+[A-Z]*)\\s+of\\s+(?:the\\s+)?(?:ICA|Indian\\s+Contract\\s+Act)'\n",
    "    ],\n",
    "    'Partnership_Act': [\n",
    "        r'Section\\s+(\\d+[A-Z]*(?:\\([a-z0-9]+\\))?)\\s+(?:of\\s+)?(?:the\\s+)?(?:Indian\\s+)?Partnership\\s+Act(?:,\\s*1932)?',\n",
    "        r'(\\d+[A-Z]*(?:\\([a-z0-9]+\\))?)\\s+of\\s+(?:the\\s+)?Partnership\\s+Act'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def check_pdf_type(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = \"\"\n",
    "            for page in reader.pages[:2]:\n",
    "                text += page.extract_text()\n",
    "            return 'text' if len(text.strip()) >= 100 else 'scanned'\n",
    "    except:\n",
    "        return 'scanned'\n",
    "\n",
    "def get_document_text(path):\n",
    "    ext = path.lower().split('.')[-1]\n",
    "    if ext == 'pdf':\n",
    "        if check_pdf_type(path) == 'text':\n",
    "            with open(path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                return \"\".join([page.extract_text() for page in reader.pages])\n",
    "        else:\n",
    "            images = convert_from_path(path)\n",
    "            return \"\".join([pytesseract.image_to_string(img) for img in images])\n",
    "    elif ext in ['docx', 'doc']:\n",
    "        doc = docx.Document(path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif ext == 'txt':\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "def extract_context_snippet(text, match_pos, window=250):\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    snippet = text[start:end].strip()\n",
    "    sentences = re.split(r'[.!?]\\s+', snippet)\n",
    "    relevant = [s.strip() for s in sentences if len(s.strip()) > 30]\n",
    "    if len(relevant) >= 2: return \" \".join(relevant[:2])\n",
    "    return relevant[0] if relevant else snippet[:200]\n",
    "\n",
    "def extract_sections_with_regex(text):\n",
    "    sections_found = defaultdict(list)\n",
    "    for act_name, patterns in ACT_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    section_num = re.sub(r'[^\\d\\w]', '', match.group(1))\n",
    "                    if not section_num or not section_num[0].isdigit(): continue\n",
    "                    match_pos = match.start()\n",
    "                    context = extract_context_snippet(text, match_pos)\n",
    "                    sections_found[f\"{act_name}_{section_num}\"].append({\n",
    "                        'position': match_pos, 'context': context, 'matched_text': match.group(0)\n",
    "                    })\n",
    "                except: continue\n",
    "    return sections_found\n",
    "\n",
    "def get_sentence_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# --- TRAINING EXECUTION ---\n",
    "docs_folder = 'docs'\n",
    "doc_files = [f for f in os.listdir(docs_folder) if f.endswith(('.pdf', '.PDF', '.docx', '.doc', '.txt'))]\n",
    "print(f\"Found {len(doc_files)} documents for training.\\n\")\n",
    "\n",
    "doc_sections_map = {}\n",
    "all_contexts = defaultdict(list)\n",
    "all_sections_data = {}\n",
    "\n",
    "for idx, filename in enumerate(doc_files, 1):\n",
    "    text = get_document_text(os.path.join(docs_folder, filename))\n",
    "    sections_found = extract_sections_with_regex(text)\n",
    "    if sections_found:\n",
    "        doc_sections_map[filename] = sections_found\n",
    "        for section_key, occurrences in sections_found.items():\n",
    "            all_sections_data[section_key] = all_sections_data.get(section_key, []) + occurrences\n",
    "            for occ in occurrences: all_contexts[section_key].append(occ['context'])\n",
    "    print(f\"Processed {idx}/{len(doc_files)}: {filename}\")\n",
    "\n",
    "print(\"\\nComputing Embeddings...\")\n",
    "embeddings = {k: get_sentence_embedding(\" \".join([o['context'] for o in v][:5])) for k, v in all_sections_data.items()}\n",
    "\n",
    "print(\"Building Hybrid Matrix...\")\n",
    "explicit_cooccur = defaultdict(lambda: defaultdict(int))\n",
    "for doc_name, sections_data in doc_sections_map.items():\n",
    "    keys = list(sections_data.keys())\n",
    "    for i, sec1 in enumerate(keys):\n",
    "        for sec2 in keys[i+1:]:\n",
    "            explicit_cooccur[sec1][sec2] += 1\n",
    "            explicit_cooccur[sec2][sec1] += 1\n",
    "\n",
    "semantic_sim = defaultdict(dict)\n",
    "keys = list(embeddings.keys())\n",
    "for i, sec1 in enumerate(keys):\n",
    "    for sec2 in keys[i+1:]:\n",
    "        sim = cosine_similarity(embeddings[sec1], embeddings[sec2])[0][0]\n",
    "        if sim >= 0.7:\n",
    "            semantic_sim[sec1][sec2] = round(float(sim), 3)\n",
    "            semantic_sim[sec2][sec1] = round(float(sim), 3)\n",
    "\n",
    "hybrid_cooccurrence = {}\n",
    "all_keys = set(list(explicit_cooccur.keys()) + list(semantic_sim.keys()) + list(all_contexts.keys()))\n",
    "\n",
    "for section in all_keys:\n",
    "    explicit = {k: v for k, v in explicit_cooccur.get(section, {}).items() if v >= 2}\n",
    "    semantic = semantic_sim.get(section, {})\n",
    "    contexts = list(set(all_contexts.get(section, [])))[:5]\n",
    "    if explicit or semantic or contexts:\n",
    "        hybrid_cooccurrence[section] = {\n",
    "            'explicit_cooccurrence': explicit,\n",
    "            'semantic_cooccurrence': semantic,\n",
    "            'contexts': contexts\n",
    "        }\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "with open('output/hybrid_cooccurrence.json', 'w') as f:\n",
    "    json.dump(hybrid_cooccurrence, f, indent=2)\n",
    "\n",
    "# --- LOO EVALUATION METRICS ---\n",
    "def evaluate_recommender(doc_map, hybrid_db):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING RECOMMENDER METRICS (LOO)\")\n",
    "    print(\"=\"*60)\n",
    "    hits_at_1 = 0\n",
    "    hits_at_5 = 0\n",
    "    total_tests = 0\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for doc_file, sections_data in doc_map.items():\n",
    "        actual_sections = list(sections_data.keys())\n",
    "        if len(actual_sections) < 2: continue\n",
    "            \n",
    "        for i in range(len(actual_sections)):\n",
    "            hidden = actual_sections[i]\n",
    "            inputs = actual_sections[:i] + actual_sections[i+1:]\n",
    "            \n",
    "            suggestions = {}\n",
    "            for inp in inputs:\n",
    "                if inp in hybrid_db:\n",
    "                    for rel, count in hybrid_db[inp].get('explicit_cooccurrence', {}).items():\n",
    "                        suggestions[rel] = suggestions.get(rel, 0) + (count * 1.0)\n",
    "                    for rel, score in hybrid_db[inp].get('semantic_cooccurrence', {}).items():\n",
    "                        suggestions[rel] = suggestions.get(rel, 0) + (score * 0.5)\n",
    "            \n",
    "            for inp in inputs:\n",
    "                if inp in suggestions: del suggestions[inp]\n",
    "                \n",
    "            top_preds = [x[0] for x in sorted(suggestions.items(), key=lambda x: x[1], reverse=True)]\n",
    "            total_tests += 1\n",
    "            \n",
    "            if top_preds:\n",
    "                if top_preds[0] == hidden: hits_at_1 += 1\n",
    "                if hidden in top_preds[:5]:\n",
    "                    hits_at_5 += 1\n",
    "                    reciprocal_ranks.append(1/(top_preds.index(hidden) + 1))\n",
    "                else: reciprocal_ranks.append(0)\n",
    "            else: reciprocal_ranks.append(0)\n",
    "\n",
    "    if total_tests > 0:\n",
    "        print(f\"Precision@1: {hits_at_1 / total_tests:.2%}\")\n",
    "        print(f\"Recall@5:    {hits_at_5 / total_tests:.2%}\")\n",
    "        print(f\"MRR:         {np.mean(reciprocal_ranks):.4f}\")\n",
    "    else:\n",
    "        print(\"Not enough data to evaluate.\")\n",
    "\n",
    "evaluate_recommender(doc_sections_map, hybrid_cooccurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8e4c3-fbcf-4c73-a464-c10623483f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
